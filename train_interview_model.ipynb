{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 library install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install dataclasses\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WanDB 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 load 및 전역변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from dataclasses import dataclass\n",
    "import json, os, random, logging, math, copy\n",
    "import numpy as np\n",
    "\n",
    "IGNORE_INDEX = -100 # 학습 loss 계산에 무시되는 index\n",
    "os.environ['WANDB_PROJECT'] = 'TEST' # wandb project 이름 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초기 random 함수 seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed 설정 함수\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 load 및 입력 형태에 맞게 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory_path):\n",
    "    filenames = os.listdir(directory_path)\n",
    "    datas = []\n",
    "    for filename in filenames:\n",
    "\n",
    "        with open(os.path.join(directory_path,filename),'r',encoding='utf8')as f:\n",
    "            datas.append(json.loads(f.read()))\n",
    "\n",
    "\n",
    "    print(f\"loading finished : {len(datas)} datas\")\n",
    "    return datas\n",
    "\n",
    "\n",
    "def data_transform(datas):\n",
    "    prompt = (\n",
    "        \"당신은 면접관입니다. 다음 명령에 따라 적절한 질문을 수행하세요.\\n\"\n",
    "        \"화자의 응답 기록을 참고하여 주제에 관련된 적절한 질문을 생성하세요.\\n\"\n",
    "        \"### 주제:\\n{intent}\\n\\n### 화자의 응답 기록:\\n{answer}\\n\\n### 질문 :\\n\"\n",
    "    )\n",
    "\n",
    "    dataset = []\n",
    "    for session in datas:\n",
    "        before_answer = None\n",
    "\n",
    "        for turn in session:\n",
    "            if before_answer is None:\n",
    "                before_answer = turn['answer']\n",
    "                continue\n",
    "            else:\n",
    "                source = prompt.format_map(dict(\n",
    "                    answer=before_answer,\n",
    "                    intent=turn['rule_based_intent']\n",
    "                ))\n",
    "                target = turn['question']\n",
    "                dataset.append(dict(\n",
    "                    source=source,\n",
    "                    target=target\n",
    "                ))\n",
    "\n",
    "                before_answer = turn['answer']\n",
    "\n",
    "    print(f\"total data samples : {len(dataset)}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing, label설정 등과 같은 전처리를 수행하는 함수\n",
    "def preprocess(sources, targets, tokenizer):\n",
    "    # 입력 및 출력을 하나로 연결해서 example을 생성\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "    # 토크나이징 수행\n",
    "    input_ids = tokenizer(text=examples, padding=False, return_attention_mask=False, return_length=False,\n",
    "                          max_length=tokenizer.model_max_length, truncation=True, verbose=False)[\"input_ids\"]\n",
    "    # 입력 부분을 복사하여 target으로 사용할 label 생성\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "\n",
    "    # 오류 체크\n",
    "    for pieces in input_ids:\n",
    "        assert not any([math.isnan(piece) or math.isinf(piece) for piece in pieces])\n",
    "\n",
    "    # 입력을 토크나이징하여, 입력부분의 길이를 계산\n",
    "    source_lens = tokenizer(text=sources, padding=False, return_attention_mask=False, return_length=True,\n",
    "                            max_length=tokenizer.model_max_length, truncation=True, verbose=False)[\"length\"]\n",
    "\n",
    "    # label의 입력 부분에 대해 loss계산을 하지 않도록 IGNORE_INDEX로 설정\n",
    "    for example_index in range(len(examples)):\n",
    "        for index in range(source_lens[example_index]):\n",
    "            labels[example_index][index] = IGNORE_INDEX\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "# Dataset 객체를 상속한 클래스 - 모델의 입출력을 가져오기 위한 단위? 로 생각하면 편할듯\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        sources = [example['source'] for example in examples]\n",
    "        targets = [f\"{example['target']}{tokenizer.eos_token}\" for example in examples]\n",
    "\n",
    "        logging.warning(msg=\"tokenizing...\")\n",
    "        data_dict = preprocess(sources=sources, targets=targets, tokenizer=tokenizer)\n",
    "        logging.warning(msg=\"tokenizing finished\")\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def naive__getitem__(self, i):\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return dict(input_ids=self.input_ids[idx], labels=self.labels[idx])\n",
    "\n",
    "# batch 단위를 처리하기 위한 collator function으로 배치내 데이터의 길이를 맞춰주는 padding 처리 등을 수행\n",
    "@dataclass\n",
    "class CustomCollator(object):\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        # 이미 tensor일 거 같긴하지만, 혹시 몰라 tensor로 변환\n",
    "        input_ids = [torch.tensor(piece) for piece in input_ids]\n",
    "        labels = [torch.tensor(piece) for piece in labels]\n",
    "\n",
    "        # 일부로 패딩을 left에 주기 위해 flip을 통해 뒤집기를 수행\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence([i.flip(dims=[-1]) for i in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id).flip(dims=[1])\n",
    "        labels = torch.nn.utils.rnn.pad_sequence([i.flip(dims=[-1]) for i in labels], batch_first=True, padding_value=IGNORE_INDEX).flip(dims=[1])\n",
    "\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 설정 및 학습 설정 argument 객체 반환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Args 객체를 반환하는 함수 - 학습에 사용되는 파라미터\n",
    "def get_training_args(args):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args['output_dir'],\n",
    "        evaluation_strategy=\"no\",\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        weight_decay=args[\"weight_decay\"],\n",
    "        push_to_hub=False,\n",
    "        do_train=True,\n",
    "        num_train_epochs=args['num_epochs'],\n",
    "        per_device_train_batch_size=args[\"batch_size\"],\n",
    "        logging_steps=args[\"logging_steps\"],\n",
    "        gradient_accumulation_steps=args[\"accumulation_steps\"],\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=args[\"save_steps\"],\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type='cosine',\n",
    "        max_grad_norm=1.0,\n",
    "        fp16=False,\n",
    "        report_to=args[\"report_to\"],\n",
    "        run_name=args[\"run_name\"],\n",
    "    )\n",
    "\n",
    "    return training_args\n",
    "\n",
    "def get_lora_args(args):\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=args['lora_r']*2,\n",
    "        lora_dropout=args['lora_dropout'],\n",
    "        r=args['lora_r'],\n",
    "        bias=args['bias'],\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    return peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(config):\n",
    "    # model and tokenizer load\n",
    "    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=config['pretrained_model_name_or_path'],\n",
    "                                                 trust_remote_code=config['trust_remote_code'],\n",
    "                                                 cache_dir=config['cache_dir'],\n",
    "                                                 local_files_only=config['local_files_only'])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=config['pretrained_model_name_or_path'],\n",
    "                                              trust_remote_code=config['trust_remote_code'],\n",
    "                                              cache_dir=config['cache_dir'],\n",
    "                                              local_files_only=config['local_files_only'],\n",
    "                                              padding_side=config['padding_side'])\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.model_max_length = config['max_token_length']\n",
    "\n",
    "    # LoRA 적용\n",
    "    lora_config = get_lora_args(config['lora_args'])\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # org dataset load\n",
    "    train_dataset = load_dataset(config['train_data_path'])\n",
    "    train_dataset = data_transform(train_dataset)\n",
    "\n",
    "\n",
    "    # prepare train dataset\n",
    "    train_dataset = CustomDataset(examples=train_dataset, tokenizer=tokenizer)\n",
    "    data_collator = CustomCollator(tokenizer=tokenizer)\n",
    "\n",
    "    # prepare training model\n",
    "    training_args = get_training_args(config['training_args'])\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_dataset,\n",
    "                      data_collator=data_collator)\n",
    "\n",
    "    # 학습 수행\n",
    "    trainer.train()\n",
    "\n",
    "    # 맨 마지막 - 학습 종료 이후 저장하는 부분\n",
    "    trainer.save_state()\n",
    "    trainer.save_model(output_dir=os.path.join(config['output_dir'],\"final\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main 문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(\"../drive/MyDrive/LoRA_tuning\")\n",
    "input_dir = os.path.join(root_dir, \"inputs\")\n",
    "\n",
    "# 반드시 경로 알잘딱 바꿔주기\n",
    "model_name = \"EleutherAI/polyglot-ko-1.3b\" # beomi/llama-2-ko-7b , EleutherAI/polyglot-ko-1.3b\n",
    "output_dir = os.path.join(root_dir, \"outputs\", \"interview\", model_name.split(\"/\")[1], \"test\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cache_dir = os.path.join(root_dir, 'cache')\n",
    "\n",
    "set_seed(seed=42)\n",
    "\n",
    "config = {\n",
    "    \"training_args\":{\n",
    "        \"output_dir\": output_dir,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"batch_size\": 8,\n",
    "        \"accumulation_steps\": 32,\n",
    "        \"logging_steps\": 1,\n",
    "        \"save_steps\": 100,\n",
    "        \"num_epochs\": 20,\n",
    "        \"report_to\": \"wandb\",\n",
    "        \"run_name\": \"session_10000\"\n",
    "    },\n",
    "    \"lora_args\": {\n",
    "        \"lora_r\": 128,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"bias\": \"none\"\n",
    "    },\n",
    "\n",
    "\n",
    "    \"pretrained_model_name_or_path\": model_name,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"cache_dir\": cache_dir,\n",
    "    \"local_files_only\": False,\n",
    "    \"padding_side\": \"left\",\n",
    "    \"max_token_length\": 1024,\n",
    "\n",
    "    \"train_data_path\": input_dir,\n",
    "    \"output_dir\": output_dir\n",
    "\n",
    "}\n",
    "\n",
    "training(config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
